[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "index",
    "section": "",
    "text": "about: links: - text Lab3 href: https://avery-eastman.github.io/csu-ess-lab3/"
  },
  {
    "objectID": "lab-03.html",
    "href": "lab-03.html",
    "title": "Lab 3: COVID-19",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose"
  },
  {
    "objectID": "lab-03.html#question-1",
    "href": "lab-03.html#question-1",
    "title": "Lab 3: COVID-19",
    "section": "Question 1",
    "text": "Question 1\n\nTake a moment to reflect on the value of open data: How does easy access to historical and real-time environmental data shape our understanding of climate trends, resource management, and public health? What happens when this data disappears or becomes inaccessible?\nAccess to both historical and real-time environmental data is essential to tracking climate, resource, and health patterns and changes over time. Which is important for addressing emerging issues, as this data provides insight on what might be done or how things will progress. Access to this information will allow scientist, governments, and other stakeholders to make informed decisions about current issues. For example, tracking the usage of resources has helped us set limits to their usage so they can be sustained. The lose of this data could lead to damaging consequences. Without being informed on past patterns or changes we might not be able to address issues effectively. Furthermore, if this data became inaccessible many people would feel increasing anxiity and a lack of transparency about issues."
  },
  {
    "objectID": "lab-03.html#question-2",
    "href": "lab-03.html#question-2",
    "title": "Lab 3: COVID-19",
    "section": "Question 2",
    "text": "Question 2\n\nStart by reading in the data from the NY-Times URL with read_csv (make sure to attach the tidyverse)\n\nurl = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'\ncovid = read_csv(url)\n\nRows: 2502832 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): county, state, fips\ndbl  (2): cases, deaths\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCreate an object called my.date and set it as “2022-02-01” - ensure this is a date object:. Create a object called my.state and set it to “Colorado”\n\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\nclass(my.date)\n\n[1] \"Date\"\n\nclass(my.state)\n\n[1] \"character\"\n\n\n\n\nStart by making a subset that limits (filter) the data to Colorado and add a new column (mutate) with the daily new cases using diff/lag by county (group_by). Do the same for new deaths as well\n\ncolorado_data &lt;- covid |&gt;\n  filter(state == my.state) |&gt;\n  arrange(county, date) \n\ncolorado_data &lt;- colorado_data |&gt;\n  group_by(county) |&gt;\n  mutate(\n    daily_new_cases = cases - lag(cases, default = first(cases)),\n    daily_new_deaths = deaths - lag(deaths, default = first(deaths))\n  ) |&gt;\n  ungroup()\n\n\n\nUsing your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases, and the second should show the 5 counties with the most NEW cases. Remember to use your my.date object as a proxy for today’s date. Your tables should have clear column names and descriptive captions\n\nmost_cumulative_cases &lt;- colorado_data |&gt;\n  group_by(county) |&gt;\n  summarise(cumulative_cases = max(cases, na.rm = TRUE)) |&gt;\n  arrange(desc(cumulative_cases)) |&gt;\n  slice_head(n = 5) |&gt;\n  ungroup()\n\nmost_new_cases &lt;- colorado_data |&gt;\n  filter(date == my.date) |&gt;\n  group_by(county) |&gt;\n  summarise(new_cases = sum(daily_new_cases, na.rm = TRUE)) |&gt;\n  arrange(desc(new_cases)) |&gt;\n  slice_head(n = 5) |&gt;\n  ungroup()\n\nft_cumulative &lt;- flextable(most_cumulative_cases) |&gt;\n  set_header_labels(county = \"County\", cumulative_cases = \"Cumulative Cases\") |&gt;\n  set_caption(as_paragraph(\"Top 5 Counties in Colorado with the Most Cumulative COVID-19 Cases\"))\nft_cumulative\n\nCountyCumulative CasesEl Paso190,541Denver177,033Arapahoe158,561Adams137,720Jefferson128,067\n\nft_new_cases &lt;- flextable(most_new_cases) |&gt;\n  set_header_labels(county = \"County\", new_cases = \"New Cases\") |&gt;\n  set_caption(as_paragraph(\"Top 5 Counties in Colorado with the Most New COVID-19 Cases\"))\nft_new_cases\n\nCountyNew CasesEl Paso630Arapahoe401Denver389Adams326Jefferson291"
  },
  {
    "objectID": "lab-03.html#question-3",
    "href": "lab-03.html#question-3",
    "title": "Lab 3: COVID-19",
    "section": "Question 3",
    "text": "Question 3\n\nGiven the above URL, and guidelines on string concatenation and formatting, read in the population data and (1) create a five digit FIP variable and only keep columns that contain “NAME” or “2021” (remember the tidyselect option found with ?dplyr::select). Additionally, remove all state level rows (e.g. COUNTY FIP == “000”)\n\npop_url &lt;- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'\ncovid_pop &lt;- read_csv(pop_url)\n\nRows: 3195 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SUMLEV, STATE, COUNTY, STNAME, CTYNAME\ndbl (62): REGION, DIVISION, ESTIMATESBASE2020, POPESTIMATE2020, POPESTIMATE2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop_data &lt;- covid_pop |&gt;\n  mutate(fips = paste0(STATE, COUNTY)) |&gt;\n  filter(COUNTY != \"000\") |&gt;\n  select(fips, contains(\"NAME\"), contains(\"2021\")) \n\n\n\nNow, explore the data … what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions… In a few sentences describe the data obtained after modification\n\nnames(pop_data)\n\n [1] \"fips\"                  \"STNAME\"                \"CTYNAME\"              \n [4] \"POPESTIMATE2021\"       \"NPOPCHG2021\"           \"BIRTHS2021\"           \n [7] \"DEATHS2021\"            \"NATURALCHG2021\"        \"INTERNATIONALMIG2021\" \n[10] \"DOMESTICMIG2021\"       \"NETMIG2021\"            \"RESIDUAL2021\"         \n[13] \"GQESTIMATES2021\"       \"RBIRTH2021\"            \"RDEATH2021\"           \n[16] \"RNATURALCHG2021\"       \"RINTERNATIONALMIG2021\" \"RDOMESTICMIG2021\"     \n[19] \"RNETMIG2021\"          \n\ndim(pop_data)\n\n[1] 3144   19\n\nnrow(pop_data)\n\n[1] 3144\n\nstr(pop_data)\n\ntibble [3,144 × 19] (S3: tbl_df/tbl/data.frame)\n $ fips                 : chr [1:3144] \"01001\" \"01003\" \"01005\" \"01007\" ...\n $ STNAME               : chr [1:3144] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ CTYNAME              : chr [1:3144] \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ POPESTIMATE2021      : num [1:3144] 59203 239439 24533 22359 59079 ...\n $ NPOPCHG2021          : num [1:3144] 288 6212 -436 171 -28 ...\n $ BIRTHS2021           : num [1:3144] 686 2337 270 240 654 ...\n $ DEATHS2021           : num [1:3144] 696 2948 390 325 875 ...\n $ NATURALCHG2021       : num [1:3144] -10 -611 -120 -85 -221 -49 -70 -593 -200 -188 ...\n $ INTERNATIONALMIG2021 : num [1:3144] 15 105 0 1 9 1 5 12 22 7 ...\n $ DOMESTICMIG2021      : num [1:3144] 242 6972 -313 254 141 ...\n $ NETMIG2021           : num [1:3144] 257 7077 -313 255 150 ...\n $ RESIDUAL2021         : num [1:3144] 41 -254 -3 1 43 4 5 86 21 2 ...\n $ GQESTIMATES2021      : num [1:3144] 484 3351 2248 1994 616 ...\n $ RBIRTH2021           : num [1:3144] 11.62 9.89 10.91 10.78 11.07 ...\n $ RDEATH2021           : num [1:3144] 11.8 12.5 15.8 14.6 14.8 ...\n $ RNATURALCHG2021      : num [1:3144] -0.169 -2.585 -4.848 -3.816 -3.74 ...\n $ RINTERNATIONALMIG2021: num [1:3144] 0.254 0.4443 0 0.0449 0.1523 ...\n $ RDOMESTICMIG2021     : num [1:3144] 4.1 29.5 -12.65 11.4 2.39 ...\n $ RNETMIG2021          : num [1:3144] 4.35 29.95 -12.65 11.45 2.54 ...\n\nglimpse(pop_data)\n\nRows: 3,144\nColumns: 19\n$ fips                  &lt;chr&gt; \"01001\", \"01003\", \"01005\", \"01007\", \"01009\", \"01…\n$ STNAME                &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala…\n$ CTYNAME               &lt;chr&gt; \"Autauga County\", \"Baldwin County\", \"Barbour Cou…\n$ POPESTIMATE2021       &lt;dbl&gt; 59203, 239439, 24533, 22359, 59079, 10143, 18890…\n$ NPOPCHG2021           &lt;dbl&gt; 288, 6212, -436, 171, -28, -86, -135, -565, -163…\n$ BIRTHS2021            &lt;dbl&gt; 686, 2337, 270, 240, 654, 111, 227, 1250, 392, 2…\n$ DEATHS2021            &lt;dbl&gt; 696, 2948, 390, 325, 875, 160, 297, 1843, 592, 4…\n$ NATURALCHG2021        &lt;dbl&gt; -10, -611, -120, -85, -221, -49, -70, -593, -200…\n$ INTERNATIONALMIG2021  &lt;dbl&gt; 15, 105, 0, 1, 9, 1, 5, 12, 22, 7, 20, 1, 0, -1,…\n$ DOMESTICMIG2021       &lt;dbl&gt; 242, 6972, -313, 254, 141, -42, -75, -70, -6, 28…\n$ NETMIG2021            &lt;dbl&gt; 257, 7077, -313, 255, 150, -41, -70, -58, 16, 29…\n$ RESIDUAL2021          &lt;dbl&gt; 41, -254, -3, 1, 43, 4, 5, 86, 21, 2, 18, 7, 8, …\n$ GQESTIMATES2021       &lt;dbl&gt; 484, 3351, 2248, 1994, 616, 1578, 285, 5407, 856…\n$ RBIRTH2021            &lt;dbl&gt; 11.615503, 9.888589, 10.908650, 10.775136, 11.06…\n$ RDEATH2021            &lt;dbl&gt; 11.78483, 12.47392, 15.75694, 14.59133, 14.80717…\n$ RNATURALCHG2021       &lt;dbl&gt; -0.1693222, -2.5853351, -4.8482890, -3.8161941, …\n$ RINTERNATIONALMIG2021 &lt;dbl&gt; 0.25398330, 0.44428836, 0.00000000, 0.04489640, …\n$ RDOMESTICMIG2021      &lt;dbl&gt; 4.0975973, 29.5007468, -12.6459537, 11.4036860, …\n$ RNETMIG2021           &lt;dbl&gt; 4.3515806, 29.9450352, -12.6459537, 11.4485824, …\n\n\nThe modified dataset has 3,144 rows and 18 columns. Generally, the columns include state & county name, county fip codes and population for 2021.\n\n\nWhat is the range of populations seen in Colorado counties in 2021\n\nmodified_CO_pop_data &lt;- pop_data |&gt;\n  filter(STNAME == \"Colorado\")\n\nrange_pop &lt;- range(modified_CO_pop_data$POPESTIMATE2021, na.rm = TRUE)\nrange_pop\n\n[1]    741 737287\n\n\nThe range of populations seen in Colorado counties in 2021 is between 741 to 737,287.\n\n\nJoin the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths\n\nper_capita_COVID_data &lt;- colorado_data |&gt;\n  inner_join(pop_data, by = \"fips\") |&gt;\n  mutate(per_capita_cum_cases = cases / POPESTIMATE2021,\n         per_capita_new_cases = daily_new_cases / POPESTIMATE2021,\n         per_capita_new_deaths = daily_new_deaths / POPESTIMATE2021)\n\n\n\nGenerate (2) new tables. The first should show the 5 counties with the most cumulative cases per capita on 2021-01-01, and the second should show the 5 counties with the most NEW cases per capita on the same date. Your tables should have clear column names and descriptive captions.\n\ndata_jan_1 &lt;-per_capita_COVID_data |&gt;\n  filter(date == \"2021-01-01\")\n\nmost_cumulative_cases_per_capita &lt;- data_jan_1 |&gt;\n  arrange(desc(per_capita_cum_cases)) |&gt;\n  select(county, per_capita_cum_cases) |&gt;\n  slice_head(n = 5)\n\nft_cumulative_per_capita &lt;- flextable(most_cumulative_cases_per_capita) |&gt;\n  set_header_labels(county = \"County\", per_capita_cum_cases = \"Per Capita Cumulative Cases\") |&gt;\n  set_caption(\"Top 5 Counties with the Most Cumulative COVID-19 Cases Per Captia on January 1st, 2021\")\nft_cumulative_per_capita\n\nCountyPer Capita Cumulative CasesCrowley0.28945074Bent0.21090092Logan0.15467009Lincoln0.14306596Fremont0.09507078\n\nmost_new_cases_per_capita &lt;- data_jan_1 |&gt;\n  arrange(desc(per_capita_new_cases)) |&gt;\n  select(county, per_capita_new_cases) |&gt;\n  slice_head(n = 5)\n\nft_new_per_capita &lt;- flextable(most_new_cases_per_capita) |&gt;\n  set_header_labels(county = \"County\", per_capita_new_cases = \"Per Capita New Cases\") |&gt;\n  set_caption(\"Top 5 Counties with the Most New COVID-19 Cases Per Capita on January 1st, 2021\")\nft_new_per_capita\n\nCountyPer Capita New CasesBent0.016669788Sedgwick0.003439381Chaffee0.002634112Crowley0.002266783Mineral0.002152853"
  },
  {
    "objectID": "lab-03.html#question-4",
    "href": "lab-03.html#question-4",
    "title": "Lab 3: COVID-19",
    "section": "Question 4",
    "text": "Question 4\n\nFilter the merged COVID/Population data to only include the last 14 days. Remember this should be a programmatic request and not hard-coded. Then, use the group_by/summarize paradigm to determine the total number of new cases in the last 14 days per 100,000 people. Print a table of the top 5 counties, and, report the number that meet the watch list condition: “More than 100 new cases per 100,000 residents over the past 14 days…”\n\nnew_cases_per_100000 &lt;- per_capita_COVID_data |&gt;\n  filter(between(date, my.date - 14, my.date)) |&gt;\n  group_by(county) |&gt;\n  summarise(total_new_cases = sum(daily_new_cases,na.rm = TRUE),\n            new_cases_per_100k = total_new_cases / POPESTIMATE2021[1] * 100000)\n\ntop_5_new_cases &lt;- new_cases_per_100000 |&gt;\n  filter(new_cases_per_100k &gt; 100) |&gt;\n  arrange(desc(new_cases_per_100k)) |&gt;\n  slice_head(n = 5) \ntop_5_new_cases\n\n# A tibble: 5 × 3\n  county  total_new_cases new_cases_per_100k\n  &lt;chr&gt;             &lt;dbl&gt;              &lt;dbl&gt;\n1 Crowley             314              5475.\n2 Lincoln             206              3764.\n3 Alamosa             623              3758.\n4 Mineral              31              3337.\n5 Conejos             253              3337.\n\n\nThe top 5 counties with the most new cases in the last 14 days per 100,000 people are: Crowley, Lincoln, Alamosa, Mineral, and Conejos.\nAll 64 counties meet the watch-list condition: “More than 100 new cases per 100,000 residents over the past 14 days…”."
  },
  {
    "objectID": "lab-03.html#question-5",
    "href": "lab-03.html#question-5",
    "title": "Lab 3: COVID-19",
    "section": "Question 5",
    "text": "Question 5\n\nGiven we are assuming it is February 1st, 2022. Your leadership has asked you to determine what percentage of deaths in each county were attributed to COVID last year (2021). From previous questions you should have a data.frame with daily COVID deaths in Colorado and the Census based, 2021 total deaths. For this question, you will find the ratio of total COVID deaths per county (2021) of all recorded deaths. In a plot of your choosing, visualize all counties where COVID deaths account for 20% or more of the annual death toll.\n\ncovid_deaths_vs_total_deaths &lt;- colorado_data |&gt;\n  left_join(pop_data, by = \"fips\") |&gt;\n  filter(year(date) == 2021) |&gt;\n  group_by(county) |&gt;\n  summarise(\n    total_covid_deaths = sum(daily_new_deaths, na.rm = TRUE),\n    covid_death_ratio = total_covid_deaths / DEATHS2021[1] * 100\n  ) |&gt;\n  select(county, covid_death_ratio) \n\nhigh_covid_death_counties &lt;- covid_deaths_vs_total_deaths |&gt;\n  filter(covid_death_ratio &gt;= 20)\n\ndeath_toll &lt;- ggplot(high_covid_death_counties, aes(x = county, y = covid_death_ratio, color = county, fill = county)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Colorado Counties with 20% or more of deaths attributed to COVID in 2021\",\n       x = \"County\",\n       y = \"COVID Death Percentage\")\n\nggsave(\"images/death_toll.png\")\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "lab-03.html#question-6",
    "href": "lab-03.html#question-6",
    "title": "Lab 3: COVID-19",
    "section": "Question 6",
    "text": "Question 6\n\nFirst, we need to group/summarize our county level data to the state level, filter it to the four states of interest, and calculate the number of daily new cases (diff/lag) and the 7-day rolling mean\n\nmulti_state_data &lt;- covid |&gt;\n  filter(state %in% c(\"New York\", \"Colorado\", \"Alabama\", \"Ohio\")) |&gt;\n  group_by(state, date) |&gt;\n  arrange(state, date) |&gt;\n  mutate(daily_new_cases = cases - lag(cases, default = first(cases)),\n         rolling_mean_7day = rollmean(daily_new_cases, k = 7, fill = NA, align = \"center\")) |&gt;\n  ungroup()\n\n\n\nUsing the modified data, make a facet plot of the daily new cases and the 7-day rolling mean. Your plot should use compelling geoms, labels, colors, and themes\n\nnew_cases_and_rolling_average &lt;- ggplot(multi_state_data, aes(x = date)) +\n  geom_bar(aes(y = daily_new_cases), stat = \"identity\", fill = \"lightblue\", alpha = 0.7) +\n  geom_line(aes(y = rolling_mean_7day), color = \"green\", linewidth = 1) +\n  facet_wrap(~state, scale = \"free_x\") +\n  labs(\n    title = \"Daily New Cases and 7-Day Rolling Average\",\n    x = \"Date\",\n    y = \"Number of New Cases\"\n  ) +\n  theme_minimal()\n\nggsave(\"images/new_cases_and_rolling_average.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 17 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\nThe story of raw case counts can be misleading. To understand why, lets explore the cases per capita of each state. To do this, join the state COVID data to the population estimates and calculate the newcases/totalpopulation Additionally, calculate the 7-day rolling mean of the new cases per capita counts\n\nmulti_state_data &lt;- covid |&gt;\n  mutate(state_fips = substr(fips, 1, 2)) |&gt;\n  filter(state %in% c(\"New York\", \"Colorado\", \"Alabama\", \"Ohio\")) |&gt;\n  group_by(state, date) |&gt;\n  arrange(state, date) |&gt;\n  mutate(daily_new_cases = cases - lag(cases, default = first(cases)),\n         rolling_mean_7day = rollmean(daily_new_cases, k = 7, fill = NA, align = \"center\")) |&gt;\n  ungroup()\n\npop_data &lt;- pop_data |&gt;\n  mutate(state_fips = substr(fips, 1, 2)) |&gt;\n  select(state_fips, contains(\"NAME\"), contains(\"2021\")) |&gt;\n  group_by(state_fips) |&gt;\n  summarise(total_population = sum(POPESTIMATE2021, na.rm = TRUE))\n\nper_capita_state_data &lt;- multi_state_data |&gt;\n  inner_join(pop_data, by = \"state_fips\") |&gt;\n  mutate(new_cases_per_capita = daily_new_cases / total_population,\n         rolling_mean_7day_per_capita = rollmean(new_cases_per_capita, k = 7, fill = NA, align = \"center\"))\n\n\n\nUsing the per capita data, plot the 7-day rolling averages overlying each other (one plot) with compelling labels, colors, and theme\n\nper_capita_new_cases_and_rolling_average &lt;- ggplot(per_capita_state_data, aes(x = date, y = rolling_mean_7day_per_capita, color = state)) +\n  geom_line(linewidth = 1.2, alpha = 0.6) +\n  labs(\n    title = \"7-Day Rolling Average of New COVID Cases per Capita by State\",\n    x = \"Date\",\n    y = \"New Cases per Capita\"\n  ) +\n  theme_minimal()\n\nggsave(\"images/per_capita_new_cases_and_rolling_average.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?\nScaling by population changes the appearance of the data to better reflect the amount of cases in each state. Without scaling by population the states with larger populations appear to have higher cases (like New York) and the states with smaller populations (like Alabama) appear to have less cases. However when changing the scaling, New York looks better than Alabama. Alabama which had the smallest amount of cases before now has the greatest amount of new cases."
  },
  {
    "objectID": "lab-03.html#question-7",
    "href": "lab-03.html#question-7",
    "title": "Lab 3: COVID-19",
    "section": "Question 7",
    "text": "Question 7\n\nFor our final task, we will explore our first spatial example! In it we will calculate the Weighted Mean Center of the COVID-19 outbreak in the USA to better understand the movement of the virus through time. To do this, we need to join the COVID data with location information. Please read in the data (readr::read_csv()); and join it to your raw COVID-19 data using the fips attributes\n\nspatial_url &lt;- 'https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv'\nspatial_data &lt;- read_csv(spatial_url)\n\nRows: 3221 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): fips\ndbl (2): LON, LAT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid_with_spatial_data &lt;- covid |&gt;\n  inner_join(spatial_data, by = \"fips\")\n\n\n\nThe mean center of a set of spatial points is defined as the average X and Y coordinate. A weighted mean center can be found by weighting the coordinates by another variable. For each date, calculate the Weighted Mean of the X and Y coordinates using the daily cumulative cases and the weight. In addition, calculate the total cases for each day, as well as the month\n\ncovid_with_spatial_data &lt;- covid_with_spatial_data |&gt;\n  mutate(month = format(date, \"%m\"))\n\nweighted_mean_center &lt;- covid_with_spatial_data |&gt;\n  group_by(date, month) |&gt;\n  summarise(\n    total_cases_daily = sum(cases, na.rm = TRUE),\n    weighted_LON = sum(LON * cases, na.rm = TRUE) / total_cases_daily,\n    weighted_LAT = sum(LAT * cases, na.rm = TRUE) / total_cases_daily,\n    .groups = \"drop\"\n  )\n\ntotal_cases_month &lt;- covid_with_spatial_data |&gt;\n  group_by(month) |&gt;\n  summarise(total_cases_monthly = sum(cases, na.rm = TRUE)) |&gt;\n  ungroup()\n\nweighted_mean_center &lt;- weighted_mean_center |&gt;\n  left_join(total_cases_month, by = \"month\")\n\n\n\nPlot the weighted mean center (aes(x = LNG, y = LAT)), colored by month, and sized by total cases for each day. These points should be plotted over a map of the USA states which can be added to a ggplot object\n\nweighted_mean_center &lt;- ggplot(weighted_mean_center, aes(x = weighted_LON, y = weighted_LAT)) +\n  borders(\"state\", fill = \"lightgray\", colour = \"white\") +\n  geom_point(aes(color = month, size = total_cases_daily), alpha = 1) +\n  labs(\n    title = \"Weighted Mean Center of COVID-19 Outbreak in the USA\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Month\",\n    size = \"Daily Total Cases\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nggsave(\"images/weighted_mean_center.png\")\n\nSaving 7 x 5 in image\n\n\n\n\nIn a few sentences, describe the movement of the COVID-19 weighted mean throughout the USA and possible drivers of its movement given your knowledge of the outbreak hot spots\nI’m not sure I graphed or compiled the data correctly, because what I would expect to see isn’t quite what my plot is showing. Within the first few months, I would expect to see the initial weighted mean center concentrated in area that had higher population density (like big cities). In the later months I would expect to see a more spread out weighted mean due to the virus having spread to rural areas and due to the measures established in different states to combat the virus."
  }
]